 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ߚ APEX-ULTRA™ AGI COSMOS: Llama 3 4-bit (any4/AWQ) Colab Deployment\n",
    "\n",
    "This notebook sets up the full AGI system with a blazing-fast, open-source Llama 3 4-bit model using vLLM. All agents and modules are ready to use the local LLM endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone Your Repo and Mount Google Drive (for persistent checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\ndrive.mount('/content/drive')\n!git clone https://github.com/YOUR_USERNAME/APEX-ULTRA.git\n%cd /content/APEX-ULTRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download a 4-bit Llama 3 Model (AWQ, any4, or NF4)\n",
    "Replace the Hugging Face link with your preferred quantized model.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install huggingface_hub\nfrom huggingface_hub import snapshot_download\nmodel_path = '/content/llama-3-8b-awq'\nsnapshot_download(repo_id='TheBloke/Llama-3-8B-AWQ', local_dir=model_path, local_dir_use_symlinks=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install vLLM, pyngrok, and All Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install vllm pyngrok python-dotenv fastapi uvicorn aiohttp nest_asyncio pandas numpy requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up .env for Llama 3 4-bit vLLM\n",
    "This cell writes a best-practice .env file for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open('.env', 'w') as f:\n    f.write('''\nLLAMA_SERVER_ENABLED=true\nLLAMA_MODEL_PATH=/content/llama-3-8b-awq\nLLAMA_SERVER_CMD=python -m vllm.entrypoints.openai.api_server --model /content/llama-3-8b-awq --host 0.0.0.0 --port 8000\nLLAMA_API_BASE=http://localhost:8000/v1\nGPT25PRO_API_KEY=\nGPT25PRO_ENDPOINT=http://localhost:8000/v1/completions\nCOLAB=true\nCHECKPOINT_PATH=/content/drive/MyDrive/agi_checkpoint.json\n''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Start the vLLM Llama 3 4-bit Server (in background)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import subprocess\nllama_proc = subprocess.Popen(\n    'python -m vllm.entrypoints.openai.api_server --model /content/llama-3-8b-awq --host 0.0.0.0 --port 8000',\n    shell=True\n)\nimport time; time.sleep(10)  # Give server time to start\nprint('vLLM Llama 3 4-bit server started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Expose the API to the Internet (ngrok)\n",
    "This gives you a public endpoint for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pyngrok import ngrok\npublic_url = ngrok.connect(8000, 'http')\nprint('Public vLLM endpoint:', public_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start the AGI Orchestrator (APEX-ULTRA™)\n",
    "This will launch the main system. All agents will use the Llama 3 4-bit endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!python3 main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**You now have a production-grade AGI system running with the best open-source 4-bit LLM!**\n",
    "\n",
    "- All agents use the Llama 3 4-bit endpoint for reasoning.\n",
    "- Checkpoints are saved to your Google Drive.\n",
    "- The API is exposed for easy integration and testing.\n",
    "\n",
    "**For advanced usage, see the repo README and .env.example.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

